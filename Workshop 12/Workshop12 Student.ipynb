{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i735RxtImEbg"
      },
      "source": [
        "#**Workshop 12: Statistical Testing**\n",
        "\n",
        "When working with machine learning models, we often run numerous experiments with various parameters. For instance, performing a grid search can yield a wide array of different results. Even small changes, such as training a model for 10 epochs instead of 9, can lead to different outcomes.\n",
        "\n",
        "This raises a crucial question: Are these results genuinely different in a meaningful way, or could they merely be due to random chance, such as differences in initialisation or other stochastic processes in the training/validation/testing paradigm?\n",
        "\n",
        "To address this, we turn to statistical testing. Statistical tests help us determine whether the observed differences in model performance are statistically significant or if they could have arisen by random chance. By applying these tests, we can gain a better understanding of the reliability and robustness of our models' results.\n",
        "\n",
        "In the following sections, we will use a permutation test and cross validation/confidence intervals to assess the statistical significance of the differences in F1 scores between our models. This approach will allow us to determine whether the observed differences are likely to be real or just random variations.\n",
        "\n",
        "You'll notice Part 1 is just like workshop 4, where we use the Wisconsin Breast Cancer Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 1 [Student, 5mins]: Load and prepare data**"
      ],
      "metadata": {
        "id": "628SvdFRrPA5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OE4hVYumEbh"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# To plot even prettier figures\n",
        "import seaborn as sn\n",
        "\n",
        "# General data handling (pure numerics are better in numpy)\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvYVFv0ImEbj"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y4WOp_hmEbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "125911db-b16b-4a4a-847e-80ecd0363541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30)\n",
            "(569,)\n",
            "(569, 31)\n"
          ]
        }
      ],
      "source": [
        "xarray = data.data\n",
        "yarray = data.target\n",
        "print(xarray.shape)\n",
        "print(yarray.shape)\n",
        "fullarray = np.concatenate((xarray,np.reshape(yarray,(-1,1))),axis=1)\n",
        "print(fullarray.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jciWhEkvmEbl"
      },
      "outputs": [],
      "source": [
        "fullarray[:,-1] = 1 - fullarray[:,-1]   # now invert the labels (so that malignant=1)\n",
        "df = pd.DataFrame(fullarray,columns = list(data.feature_names) + ['target'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwWpIzgYmEbl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "bigtrain_set, test_set = train_test_split(fullarray, test_size=0.2, random_state=42)\n",
        "train_set, val_set = train_test_split(bigtrain_set, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1NpE7gDmEbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8ef01a-bab7-4a00-b21a-3409c0138ba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(364, 30), (364,), (114, 30), (114,), (91, 30), (91,)]\n"
          ]
        }
      ],
      "source": [
        "X_bigtrain = bigtrain_set[:,:-1]\n",
        "y_bigtrain = bigtrain_set[:,-1]\n",
        "X_train = train_set[:,:-1]\n",
        "y_train = train_set[:,-1]\n",
        "X_test = test_set[:,:-1]\n",
        "y_test = test_set[:,-1]\n",
        "X_val = val_set[:,:-1]\n",
        "y_val = val_set[:,-1]\n",
        "print([X_train.shape,y_train.shape,X_test.shape,y_test.shape,X_val.shape,y_val.shape])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owUHTRAfmEbm"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "preproc_pl = Pipeline([ ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "                        ('std_scaler', StandardScaler()) ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHZVig83mEbm"
      },
      "source": [
        "#**Part 2 [Student, 20mins]: Perform Permutation Test**\n",
        "\n",
        "Run the below cells of code, where you will run three classifiers.\n",
        "\n",
        "We will run some evaluation metrics and try to assess which is best, and answer the question of how much something has to be different for it to be considered significant (i.e. how much statistical difference between evaluation metrics)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhzC8NUkmEbm"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bOakqyHmEbm"
      },
      "outputs": [],
      "source": [
        "rs = np.random.randint(100,size=1)[0]\n",
        "\n",
        "# Run three classifiers\n",
        "#-------------------------------------------------------------------------------\n",
        "#  - The first two are fundamentally the same classifier but just initialised differently.\n",
        "# Should be equally good on average (but different each time).\n",
        "\n",
        "#  - The last one should be different on average (last one gives same result each time).\n",
        "\n",
        "y_val_pred = []\n",
        "pipes = []\n",
        "\n",
        "# Initialise with random state\n",
        "pipes += [ Pipeline([ ('preproc',preproc_pl), ('sgd',SGDClassifier(loss='log_loss',random_state=rs)) ]) ]\n",
        "pipes[0].fit(X_train,y_train)\n",
        "y_val_pred +=  [ pipes[0].predict(X_val) ]\n",
        "\n",
        "pipes += [ Pipeline([ ('preproc',preproc_pl), ('sgd',SGDClassifier(loss='log_loss',random_state=rs+1)) ]) ]\n",
        "pipes[1].fit(X_train,y_train)\n",
        "y_val_pred += [ pipes[1].predict(X_val) ]\n",
        "\n",
        "# This next one is deliberately rather poor (k=200), to show a difference\n",
        "pipes += [ Pipeline([ ('preproc',preproc_pl), ('knn',KNeighborsClassifier(n_neighbors=200))]) ]\n",
        "pipes[2].fit(X_train,y_train)\n",
        "y_val_pred += [ pipes[2].predict(X_val) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YjZoMCqmEbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4a4394-cf61-410d-dfd5-3f1419e5285a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which score is better?\n",
            "Method 0: 0.932\n",
            "Method 1: 0.958\n",
            "Method 2: 0.737\n"
          ]
        }
      ],
      "source": [
        "# We go to use F1 score to evaluate the performance (just as example, could be whatever metric)\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print('Which score is better?')\n",
        "for n in range(3):\n",
        "    print(f'Method {n}: {f1_score(y_val,y_val_pred[n]):.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2x-NGMQmEbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd363550-c2b7-4bcd-b25a-5aa4b0edb8d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
              "       0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
              "       0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
              "       1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# We can check the specific results of each model\n",
        "# Because each classifier makes a prediction over lots of different samples, we\n",
        "# can use this to do some statistics.\n",
        "\n",
        "y_val_pred[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at samples where SGD classifiers have not predicted the same.\n",
        "print(y_val_pred[0]!=y_val_pred[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqCi180f0eWl",
        "outputId": "063e520c-3300-43c4-8b20-5d79f00df240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False False False False False False False False False False False False\n",
            " False False False False False False False  True False False False False\n",
            " False False False False False False  True False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at samples where first SGD classifier and KNN classifier have not predicted the same.\n",
        "print(y_val_pred[0]!=y_val_pred[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nllD4ccP0vdL",
        "outputId": "e5dd6952-6316-48f3-8554-b27a85c7070a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False False False False  True False False False False  True  True False\n",
            " False False False  True False False False  True False False  True False\n",
            " False False False  True False False  True False False False False False\n",
            " False  True False False False False False False False False False False\n",
            " False False  True False False  True False False False False False False\n",
            " False False False  True False False False False False False False False\n",
            "  True False False False False False  True False  True False False False\n",
            " False False False  True False False False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mlxtend\n",
        "We will be using a new library here:\n",
        "If you are using pip:\n",
        "* pip install mlxtend\n",
        "If you are using conda:\n",
        "* conda install -c conda-forge mlxtend"
      ],
      "metadata": {
        "id": "1Ne9X5ok5b9t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hZxgUv1mEbn"
      },
      "outputs": [],
      "source": [
        "from mlxtend.evaluate import permutation_test   # pip install mlxtend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBtjanzFmEbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe7f006-9a41-4098-a0b6-33176f065a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P value comparing methods 0 and 1: 0.489\n",
            "P value comparing methods 0 and 2: 0.002\n",
            "P value comparing methods 1 and 2: 0.001\n",
            "\n",
            "Threshold is 0.0167, where P value needs to be *below* this for significance\n"
          ]
        }
      ],
      "source": [
        "ntests = 0\n",
        "# We go to pick up one model\n",
        "for n1 in range(2):\n",
        "    # and compare with another\n",
        "    for n2 in range(n1+1,3):\n",
        "        # We go to use a  paired permutation test\n",
        "        p_value = permutation_test(\n",
        "            y_val_pred[n1], y_val_pred[n2], paired=True,\n",
        "            # Comparison step\n",
        "            func=lambda x, y: np.abs(f1_score(y_val,x) - f1_score(y_val,y)),\n",
        "            method=\"approximate\", seed=0, num_rounds=1000\n",
        "        )\n",
        "        ntests += 1\n",
        "\n",
        "        print(f'P value comparing methods {n1} and {n2}: {p_value:.3f}')\n",
        "print(f'\\nThreshold is {0.05/ntests:.4f}, where P value needs to be *below* this for significance')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing another example\n"
      ],
      "metadata": {
        "id": "LUzqh0BXAleT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6553fab-36fa-4f20-b046-829949f73261",
        "id": "RFcZo0tS8NnT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which score is better?\n",
            "Method 0: 0.932\n",
            "Method 1: 0.958\n",
            "Method 2: 0.879\n"
          ]
        }
      ],
      "source": [
        "# What if we now change our KNN model?\n",
        "y_val_pred = []\n",
        "pipes = []\n",
        "p_value = []\n",
        "\n",
        "# Initialise with random state\n",
        "pipes += [ Pipeline([ ('preproc',preproc_pl), ('sgd',SGDClassifier(loss='log_loss',random_state=rs)) ]) ]\n",
        "pipes[0].fit(X_train,y_train)\n",
        "y_val_pred +=  [ pipes[0].predict(X_val) ]\n",
        "\n",
        "pipes += [ Pipeline([ ('preproc',preproc_pl), ('sgd',SGDClassifier(loss='log_loss',random_state=rs+1)) ]) ]\n",
        "pipes[1].fit(X_train,y_train)\n",
        "y_val_pred += [ pipes[1].predict(X_val) ]\n",
        "\n",
        "pipes += [ Pipeline([ ('preproc',preproc_pl), ('knn',KNeighborsClassifier(n_neighbors=140))]) ]\n",
        "pipes[2].fit(X_train,y_train)\n",
        "y_val_pred += [ pipes[2].predict(X_val) ]\n",
        "\n",
        "# We go to use F1 score to evaluate the performance (just as example, could be whatever metric)\n",
        "print('Which score is better?')\n",
        "for n in range(3):\n",
        "    print(f'Method {n}: {f1_score(y_val,y_val_pred[n]):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform permutation test again with new value of k\n",
        "\n",
        "ntests = 0\n",
        "# We go to pick up one model\n",
        "for n1 in range(2):\n",
        "    # and compare with another\n",
        "    for n2 in range(n1+1,3):\n",
        "        # We go to use a  paired permutation test\n",
        "        p_value = permutation_test(\n",
        "            y_val_pred[n1], y_val_pred[n2], paired=True,\n",
        "            # Comparison step\n",
        "            func=lambda x, y: np.abs(f1_score(y_val,x) - f1_score(y_val,y)),\n",
        "            method=\"approximate\", seed=0, num_rounds=1000\n",
        "        )\n",
        "        ntests += 1\n",
        "\n",
        "        print(f'P value comparing methods {n1} and {n2}: {p_value:.3f}')\n",
        "print(f'\\nThreshold is {0.05/ntests:.4f}, where P value needs to be *below* this for significance')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QBD3oJe98tg",
        "outputId": "59056136-bded-4d33-c6fd-26e76ca1c3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P value comparing methods 0 and 1: 0.489\n",
            "P value comparing methods 0 and 2: 0.173\n",
            "P value comparing methods 1 and 2: 0.040\n",
            "\n",
            "Threshold is 0.0167, where P value needs to be *below* this for significance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intepretation:** The p-value helps us determine whether the evaluation metric (F1 scores) are likely to come from models with different performance.\n",
        "\n",
        "* Low p-value: If the p-value is low (typically less than 0.05), it suggests that the F1 scores are significantly different, indicating that the models perform differently.\n",
        "\n",
        "* High p-value: If the p-value is high, it suggests that the F1 scores are not significantly different, indicating that the models perform similarly.\n",
        "In essence, a low p-value indicates strong evidence against the null hypothesis (i.e., the models have similar performance), while a high p-value indicates weak evidence against the null hypothesis.\n"
      ],
      "metadata": {
        "id": "FWRNgFv72ipr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Some notes and questions**\n",
        "\n",
        "In this case, we divide the significance level by 3 (hence the 0.05/ntests). Why? When you have 3 models, you need to make three pairwise comparisons: A vs. B, A vs. C, and B vs. C. To illustrate, consider rolling dice: as you roll more dice, the likelihood of getting at least one six increases. Similarly, when performing multiple tests, the likelihood of obtaining a random result that fulfills p < 0.05 also increases. This phenomenon is known as the multiple comparisons problem. To account for this, we adjust our significance threshold to reduce the chances of a Type I error (false positive). Instead of using a significance level of 0.05 for each individual test, we use a corrected threshold of 0.05 divided by the number of tests (3 in this case), resulting in 0.0167. This correction is called the Bonferroni correction. While there are more sophisticated correction methods, Bonferroni is straightforward and intuitively easy to understand.\n",
        "\n",
        "* Looking at the P-values... are any of them small enough?\n",
        "\n",
        "* Try modifying the KNN model to have more or less neighbours. How do the p-values change?\n",
        "\n",
        "* Here is a nice visual explainer: https://www.jwilber.me/permutationtest/"
      ],
      "metadata": {
        "id": "htfLPcFu7r3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### False Discovery Rate (FDR) Correction\n",
        "\n",
        "In multiple hypothesis testing, it’s essential to adjust for the increased likelihood of obtaining false positives due to performing multiple comparisons. The example above shows a common method for this adjustment, called the Bonferroni correction. However, an alternative approach that can be used is the False Discovery Rate (FDR) correction, specifically the Benjamini-Hochberg procedure, which can be implemented using the fdrcorrection function from the statsmodels library.\n",
        "\n",
        "**Purpose:**\n",
        "The fdrcorrection method is used to control the false discovery rate, which is the expected proportion of false positives among all significant hypotheses. This approach is often preferred over the Bonferroni correction when you want to balance the need to identify true positives while controlling for false positives.\n",
        "\n",
        "**How It Works:**\n",
        "* Input P-values: You start with a list of p-values obtained from multiple hypothesis tests.\n",
        "* FDR Correction: The fdrcorrection function adjusts these p-values to control the false discovery rate. It returns two outputs:\n",
        "** 'hyptest': A list of Boolean values indicating whether each hypothesis is * significant (True) or not (False) after the correction.\n",
        "** 'pcorr': A list of the corrected p-values.\n",
        "* Interpretation: You can compare the corrected p-values (pcorr) to a standard significance threshold (e.g., 0.05) to determine which hypotheses are significant.\n",
        "\n",
        "**Comparison to Bonferroni Correction:**\n",
        "* Bonferroni Correction: This method is very conservative, as it adjusts the significance threshold by dividing it by the number of tests. This can reduce the likelihood of false positives but also increases the risk of false negatives (missing true effects).\n",
        "\n",
        "* FDR Correction: The fdrcorrection method is less conservative compared to Bonferroni. It aims to control the proportion of false positives among the rejected hypotheses, allowing for more true positives to be identified while still controlling for false discoveries.\n"
      ],
      "metadata": {
        "id": "0-t_G9v4Aoal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.multitest import fdrcorrection\n",
        "\n",
        "pvals = []\n",
        "ntests = 0\n",
        "for n1 in range(2):\n",
        "    for n2 in range(n1 + 1, 3):\n",
        "        p_value = permutation_test(\n",
        "            y_val_pred[n1], y_val_pred[n2], paired=True,\n",
        "            func=lambda x, y: np.abs(f1_score(y_val, x) - f1_score(y_val, y)),\n",
        "            method=\"approximate\", seed=0, num_rounds=1000\n",
        "        )\n",
        "        ntests += 1\n",
        "        pvals.append(p_value)\n",
        "\n",
        "# Apply FDR correction on the list of p-values\n",
        "hyptest, pcorr = fdrcorrection(pvals, method='p')\n",
        "\n",
        "print(f\"Hypothesis test results (True = different): {hyptest}\")\n",
        "print(f\"Corrected P-values (compare to standard 0.05 threshold): {pcorr}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkt3bYV9Buyl",
        "outputId": "4bf4cdf8-a79e-4c6d-8d81-de13bad7dddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypothesis test results (True = different): [False False False]\n",
            "Corrected P-values (compare to standard 0.05 threshold): [0.48851149 0.25924076 0.11988012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 3 [Student, 10mins]: Perform Cross Validation and Calculate Confidence Intervals**\n",
        "Another way to assess these statistics is by examining the confidence interval (CI) of the data. Here, we use cross-validation to generate performance metrics (such as the F1 score).\n",
        "\n",
        "After calculating our scores, we generate confidence intervals. The value '1.96' corresponds to a 95% confidence level. This means that based on the number of times we ran the test (in this case, 5), we can estimate how 'confident' we are that the mean value is accurate. The confidence interval represents the range within which we are 95% certain that the true value lies.\n",
        "\n",
        "If the confidence intervals of two models overlap, it indicates that we cannot say with certainty that the performance of the two models is distinct."
      ],
      "metadata": {
        "id": "3Hd3hhiI6RyZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l63tv26TmEbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40b9a46-03a7-462f-fab7-a25b5df6a3d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Results for method 0 ***\n",
            "  Validation results are 0.933 +/- 0.032\n",
            "*** Results for method 1 ***\n",
            "  Validation results are 0.953 +/- 0.019\n",
            "*** Results for method 2 ***\n",
            "  Validation results are 0.716 +/- 0.073\n"
          ]
        }
      ],
      "source": [
        "# Cross validation results for comparison\n",
        "#  - the P values above are the best test, but this is a useful comparison\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import fbeta_score, make_scorer\n",
        "\n",
        "f1_scorer = make_scorer(fbeta_score, beta=1)\n",
        "\n",
        "for n in range(3):\n",
        "    print(f'*** Results for method {n} ***')\n",
        "    cv_results = cross_validate(pipes[n], X_bigtrain, y_bigtrain, cv=5, return_train_score=True, scoring=f1_scorer)\n",
        "    # the following is a 95% confidence interval calculation\n",
        "    # - this is better than standard deviation as overlapping intervals => no stat significance (approx)\n",
        "    # Note that this is approximate as it assumes a normal distribution\n",
        "    #  it also does not take into account multiple tests\n",
        "    #  therefore it implies significance more easily than the P values above (which are better)\n",
        "    CI = np.std(cv_results['test_score'])*1.96/np.sqrt(cv_results['test_score'].shape[0])\n",
        "    print(f\"  Validation results are {np.mean(cv_results['test_score']):.3f} +/- {CI:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 4 [Student, 10mins]: Adjusting sample size**\n",
        "\n",
        "When analysing the performance of different machine learning models, the size of the sample (in this demonstration shown by changing the size of the validation set) can significantly impact the statistical significance of the results.\n",
        "\n",
        "Run the code below. What do you notice?\n"
      ],
      "metadata": {
        "id": "zoq5quGIDum7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "bigtrain_set, test_set = train_test_split(fullarray, test_size=0.4, random_state=42)"
      ],
      "metadata": {
        "id": "4HAy7Y7vD6rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bigtrain = bigtrain_set[:,:-1]\n",
        "y_bigtrain = bigtrain_set[:,-1]\n",
        "X_train = train_set[:,:-1]\n",
        "y_train = train_set[:,-1]\n",
        "X_test = test_set[:,:-1]\n",
        "y_test = test_set[:,-1]\n",
        "X_val = val_set[:,:-1]\n",
        "y_val = val_set[:,-1]\n",
        "print([X_train.shape,y_train.shape,X_test.shape,y_test.shape,X_val.shape,y_val.shape])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCi1Cu00EFys",
        "outputId": "0e17597c-7363-43ef-c319-024c7a25b7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(364, 30), (364,), (228, 30), (228,), (91, 30), (91,)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "preproc_pl = Pipeline([ ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "                        ('std_scaler', StandardScaler()) ])"
      ],
      "metadata": {
        "id": "K6pW6RSiEN0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs = np.random.randint(100, size=1)[0]\n",
        "\n",
        "# run three classifiers\n",
        "#  - the first two should be equally good on average (but different each time)\n",
        "#  - the last one should be different on average (last one gives same result each time)\n",
        "\n",
        "y_val_pred = []\n",
        "pipes = []\n",
        "\n",
        "pipes += [ Pipeline([ ('preproc',preproc_pl), ('sgd',SGDClassifier(loss='log_loss',random_state=rs)) ]) ]\n",
        "pipes[0].fit(X_train,y_train)\n",
        "y_val_pred +=  [ pipes[0].predict(X_val) ]\n",
        "\n",
        "pipes += [ Pipeline([ ('preproc',preproc_pl), ('sgd',SGDClassifier(loss='log_loss',random_state=rs+1)) ]) ]\n",
        "pipes[1].fit(X_train,y_train)\n",
        "y_val_pred += [ pipes[1].predict(X_val) ]\n",
        "\n",
        "# this next one is deliberately rather poor, to show a difference (n_neighbours is too high)\n",
        "pipes += [ Pipeline([ ('preproc',preproc_pl), ('knn',KNeighborsClassifier(n_neighbors=200)) ]) ]\n",
        "pipes[2].fit(X_train,y_train)\n",
        "y_val_pred += [ pipes[2].predict(X_val) ]"
      ],
      "metadata": {
        "id": "YEZB3WUbER-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print('Which score is better?')\n",
        "\n",
        "for n in range(3):\n",
        "    print(f'Method {n}: {f1_score(y_val,y_val_pred[n]):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2He-X-pETqh",
        "outputId": "50fa7f1b-4fe5-45e5-d9b6-886087e4d6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which score is better?\n",
            "Method 0: 0.958\n",
            "Method 1: 0.944\n",
            "Method 2: 0.737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pvals = []\n",
        "ntests = 0\n",
        "for n1 in range(2):\n",
        "    for n2 in range(n1+1,3):\n",
        "        p_value = permutation_test(\n",
        "            y_val_pred[n1], y_val_pred[n2], paired=True,\n",
        "            func=lambda x, y: np.abs(f1_score(y_val,x) - f1_score(y_val,y)),\n",
        "            method=\"approximate\", seed=0, num_rounds=1000\n",
        "        )\n",
        "        ntests += 1\n",
        "\n",
        "        pvals += [p_value]\n",
        "\n",
        "        print(f'P value comparing methods {n1} and {n2}: {p_value:.3f}')\n",
        "print(f'\\nThreshold is {0.05/ntests:.4f}, where P value needs to be *below* this for significance')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlBNeK8cEWiF",
        "outputId": "be3eee42-4071-43d9-e7e8-ea48d9849482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P value comparing methods 0 and 1: 1.000\n",
            "P value comparing methods 0 and 2: 0.001\n",
            "P value comparing methods 1 and 2: 0.001\n",
            "\n",
            "Threshold is 0.0167, where P value needs to be *below* this for significance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply FDR correction on the list of p-values\n",
        "hyptest, pcorr = fdrcorrection(pvals, method='p')\n",
        "\n",
        "print(f\"Hypothesis test results (True = different): {hyptest}\")\n",
        "print(f\"Corrected P-values (compare to standard 0.05 threshold): {pcorr}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4G1xSDUE7Am",
        "outputId": "fb0e2db2-181d-4a5d-8a82-d705b9b411b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypothesis test results (True = different): [False  True  True]\n",
            "Corrected P-values (compare to standard 0.05 threshold): [1.        0.0014985 0.0014985]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}